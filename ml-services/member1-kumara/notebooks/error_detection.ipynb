{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c045a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: scikit-learn in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: joblib in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\fuelwatch_project\\fuelwatch-project\\ml-services\\member1-kumara\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb1ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Commands\n",
    "\n",
    "import os, re, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d9c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "\n",
    "DATA_PATH = \"../data/raw/fuel_dispenses.csv\"   \n",
    "OUTDIR = \"rf_outputs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "STATION_ID = \"STATION_01\"\n",
    "TRAIN_QUANTILE = 0.80     # time-based split\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "REFILL_BAL_UP = 300.0   \n",
    "DROP_BAL_DOWN = 300.0    \n",
    "MISMATCH_GAP = 300.0      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae93f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "def to_float(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().replace(\",\", \"\")\n",
    "    if s == \"\" or s.lower() == \"nan\":\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def normalize_fuel(item: str) -> str:\n",
    "    s = str(item).lower()\n",
    "    if \"petrol\" in s and \"92\" in s: return \"PETROL_92\"\n",
    "    if \"petrol\" in s and \"95\" in s: return \"PETROL_95\"\n",
    "    if \"diesel\" in s and (\"auto\" in s or \"super\" in s): return \"DIESEL_AUTO\"\n",
    "    if \"diesel\" in s: return \"DIESEL\"\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", s).strip(\"_\").upper()\n",
    "\n",
    "def load_and_clean(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df[\"station_id\"] = STATION_ID\n",
    "    df[\"tank_id\"] = df[\"Site.1\"].astype(str).str.strip()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df[\"fuel_type\"] = df[\"Item\"].apply(normalize_fuel)\n",
    "\n",
    "    df[\"qty\"] = pd.to_numeric(df[\"Qty\"], errors=\"coerce\")\n",
    "    df[\"amount\"] = df[\"Amount\"].apply(to_float) if \"Amount\" in df.columns else np.nan\n",
    "    df[\"balance\"] = df[\"Balance\"].apply(to_float) if \"Balance\" in df.columns else np.nan\n",
    "\n",
    "    df = df.dropna(subset=[\"date\", \"tank_id\", \"fuel_type\", \"qty\"]).copy()\n",
    "    df = df[df[\"tank_id\"].str.lower() != \"nan\"].copy()\n",
    "    df = df.sort_values([\"station_id\", \"tank_id\", \"fuel_type\", \"date\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def build_daily_behavior(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    daily = (\n",
    "        df.groupby([\"station_id\", \"tank_id\", \"fuel_type\", df[\"date\"].dt.date])\n",
    "          .agg(\n",
    "              total_qty=(\"qty\", \"sum\"),\n",
    "              txn_count=(\"qty\", \"count\"),\n",
    "              avg_txn=(\"qty\", \"mean\"),\n",
    "              std_txn=(\"qty\", \"std\"),\n",
    "              closing_balance=(\"balance\", \"last\"),\n",
    "          )\n",
    "          .reset_index()\n",
    "          .rename(columns={\"date\": \"day\"})\n",
    "    )\n",
    "\n",
    "    daily[\"day\"] = pd.to_datetime(daily[\"day\"])\n",
    "    daily = daily.sort_values([\"station_id\", \"tank_id\", \"fuel_type\", \"day\"]).reset_index(drop=True)\n",
    "\n",
    "    daily[\"std_txn\"] = daily[\"std_txn\"].fillna(0.0)\n",
    "    daily[\"dow\"] = daily[\"day\"].dt.dayofweek\n",
    "    daily[\"is_weekend\"] = (daily[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # Stock movement\n",
    "    daily[\"balance_delta\"] = daily.groupby([\"station_id\",\"tank_id\",\"fuel_type\"])[\"closing_balance\"].diff().fillna(0.0)\n",
    "\n",
    "    # Dispense vs stock movement mismatch proxy\n",
    "    daily[\"qty_vs_balance_gap\"] = (daily[\"total_qty\"] - daily[\"balance_delta\"].abs()).fillna(0.0)\n",
    "\n",
    "    # Optional rolling context (helps RF a lot)\n",
    "    g = daily.groupby([\"station_id\",\"tank_id\",\"fuel_type\"])\n",
    "    daily[\"qty_7d_mean\"] = g[\"total_qty\"].rolling(7, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
    "    daily[\"qty_7d_std\"]  = g[\"total_qty\"].rolling(7, min_periods=1).std().fillna(0.0).reset_index(level=[0,1,2], drop=True)\n",
    "\n",
    "    daily[\"gap_7d_mean\"] = g[\"qty_vs_balance_gap\"].rolling(7, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
    "    daily[\"gap_7d_std\"]  = g[\"qty_vs_balance_gap\"].rolling(7, min_periods=1).std().fillna(0.0).reset_index(level=[0,1,2], drop=True)\n",
    "\n",
    "    return daily\n",
    "\n",
    "def make_labels_percentile(daily: pd.DataFrame, top_q=0.30):\n",
    "    d = daily.copy()\n",
    "\n",
    "    # a simple combined score\n",
    "    d[\"rule_score\"] = (\n",
    "        d[\"balance_delta\"].abs().fillna(0.0)\n",
    "        + d[\"qty_vs_balance_gap\"].abs().fillna(0.0)\n",
    "        + d[\"std_txn\"].fillna(0.0)\n",
    "    )\n",
    "\n",
    "    # label top_q per tank/fuel\n",
    "    g = d.groupby([\"station_id\",\"tank_id\",\"fuel_type\"])[\"rule_score\"]\n",
    "    thresh = g.transform(lambda x: x.quantile(1 - top_q))\n",
    "\n",
    "    d[\"label\"] = (d[\"rule_score\"] >= thresh).astype(int)\n",
    "    d[\"reason\"] = np.where(d[\"label\"] == 1, \"Top-score irregular\", \"Normal\")\n",
    "    return d\n",
    "\n",
    "def plot_label_distribution(d, outpath):\n",
    "    counts = d[\"label\"].value_counts().sort_index()\n",
    "    plt.figure()\n",
    "    plt.bar([\"Normal(0)\", \"Irregular(1)\"], [counts.get(0,0), counts.get(1,0)])\n",
    "    plt.ylabel(\"Count of days\")\n",
    "    plt.title(\"Label Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath)\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, outpath, top_n=15):\n",
    "    imp = model.feature_importances_\n",
    "    idx = np.argsort(imp)[::-1][:top_n]\n",
    "    plt.figure()\n",
    "    plt.bar(range(len(idx)), imp[idx])\n",
    "    plt.xticks(range(len(idx)), [feature_names[i] for i in idx], rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.title(f\"Top {top_n} Feature Importances (Random Forest)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath)\n",
    "    plt.close()\n",
    "\n",
    "def plot_prob_timeline(d_test, prob, outpath):\n",
    "    tmp = d_test.copy()\n",
    "    tmp[\"prob_irregular\"] = prob\n",
    "    tmp = tmp.sort_values(\"day\")\n",
    "    plt.figure()\n",
    "    plt.plot(tmp[\"day\"].values, tmp[\"prob_irregular\"].values)\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Probability of Irregular\")\n",
    "    plt.title(\"Irregularity Probability Over Time (Test Set)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2776773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training & Tunning the model\n",
    "\n",
    "FEATURES = [\n",
    "    \"total_qty\",\"txn_count\",\"avg_txn\",\"std_txn\",\n",
    "    \"balance_delta\",\"qty_vs_balance_gap\",\n",
    "    \"dow\",\"is_weekend\",\n",
    "    \"qty_7d_mean\",\"qty_7d_std\",\n",
    "    \"gap_7d_mean\",\"gap_7d_std\"\n",
    "]\n",
    "\n",
    "def train_evaluate_once(daily_labeled: pd.DataFrame):\n",
    "    # time split\n",
    "    cutoff = daily_labeled[\"day\"].quantile(TRAIN_QUANTILE)\n",
    "    train_df = daily_labeled[daily_labeled[\"day\"] <= cutoff].copy()\n",
    "    test_df  = daily_labeled[daily_labeled[\"day\"] >  cutoff].copy()\n",
    "\n",
    "    X_train = train_df[FEATURES].fillna(0.0).values\n",
    "    y_train = train_df[\"label\"].values.astype(int)\n",
    "\n",
    "    X_test  = test_df[FEATURES].fillna(0.0).values\n",
    "    y_test  = test_df[\"label\"].values.astype(int)\n",
    "\n",
    "    # Scaling helps some RF splits when features vary widely (optional but ok)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "    # Random Forest (balanced handles class imbalance)\n",
    "    rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_s, y_train)\n",
    "\n",
    "    pred = rf.predict(X_test_s)\n",
    "    # Get probabilities\n",
    "    proba = rf.predict_proba(X_test_s)\n",
    "    if proba.shape[1] == 2:\n",
    "        prob = proba[:, 1]\n",
    "    else:\n",
    "        prob = np.zeros(len(X_test_s))\n",
    "\n",
    "# ðŸ”§ Custom decision threshold\n",
    "    threshold = 0.65\n",
    "    pred = (prob >= threshold).astype(int)\n",
    "\n",
    "# Metrics\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    prec = precision_score(y_test, pred, zero_division=0)\n",
    "    rec = recall_score(y_test, pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, pred, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"rf\": rf,\n",
    "        \"scaler\": scaler,\n",
    "        \"train_df\": train_df,\n",
    "        \"test_df\": test_df,\n",
    "        \"y_test\": y_test,\n",
    "        \"pred\": pred,\n",
    "        \"prob\": prob,\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": float(acc),\n",
    "            \"precision\": float(prec),\n",
    "            \"recall\": float(rec),\n",
    "            \"f1\": float(f1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def pick_best_config_percentile(daily: pd.DataFrame, target_low=0.70, target_high=0.80):\n",
    "    topq_grid = [0.05, 0.10, 0.15, 0.20, 0.30, 0.40]\n",
    "\n",
    "    best_in_range = None\n",
    "    best_close = None\n",
    "\n",
    "    for q in topq_grid:\n",
    "        labeled = make_labels_percentile(daily, top_q=q)\n",
    "\n",
    "        # ensure both classes exist\n",
    "        if labeled[\"label\"].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        out = train_evaluate_once(labeled)\n",
    "        acc = out[\"metrics\"][\"accuracy\"]\n",
    "\n",
    "        # store label setting\n",
    "        out[\"label_thresholds\"] = {\"top_q\": q}\n",
    "\n",
    "        if target_low <= acc <= target_high:\n",
    "            if best_in_range is None or acc > best_in_range[\"metrics\"][\"accuracy\"]:\n",
    "                best_in_range = out\n",
    "\n",
    "        dist = min(abs(acc - target_low), abs(acc - target_high)) if (acc < target_low or acc > target_high) else 0\n",
    "        if best_close is None or dist < best_close[\"dist\"]:\n",
    "            best_close = out\n",
    "            best_close[\"dist\"] = dist\n",
    "\n",
    "    return best_in_range if best_in_range is not None else best_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4166c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load & clean\n",
      "Build daily behavior dataset\n",
      "Tune label thresholds & train RF \n",
      "Balanced Accuracy: 0.707290767903365\n",
      "\n",
      "Selected Label Thresholds: {'top_q': 0.3}\n",
      "Test Metrics: {'accuracy': 0.7676767676767676, 'precision': 0.8947368421052632, 'recall': 0.4473684210526316, 'f1': 0.5964912280701754}\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Normal(0)       0.74      0.97      0.84        61\n",
      "Irregular(1)       0.89      0.45      0.60        38\n",
      "\n",
      "    accuracy                           0.77        99\n",
      "   macro avg       0.82      0.71      0.72        99\n",
      "weighted avg       0.80      0.77      0.74        99\n",
      "\n",
      "\n",
      "âœ… DONE. Outputs saved in: rf_outputs\n",
      "Key charts:\n",
      "- label_distribution.png, confusion_matrix.png, roc_curve.png (if valid), pr_curve.png (if valid),\n",
      "- feature_importance.png, probability_timeline.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def main():\n",
    "    print(\"Load & clean\")\n",
    "    raw = load_and_clean(DATA_PATH)\n",
    "    raw.to_csv(os.path.join(OUTDIR, \"raw_cleaned.csv\"), index=False)\n",
    "\n",
    "    print(\"Build daily behavior dataset\")\n",
    "    daily = build_daily_behavior(raw)\n",
    "    daily.to_csv(os.path.join(OUTDIR, \"daily_behavior_dataset.csv\"), index=False)\n",
    "\n",
    "    print(\"Tune label thresholds & train RF \")\n",
    "    best = pick_best_config_percentile(daily, target_low=0.70, target_high=0.80)\n",
    "\n",
    "    rf = best[\"rf\"]\n",
    "    scaler = best[\"scaler\"]\n",
    "    train_df = best[\"train_df\"]\n",
    "    test_df = best[\"test_df\"]\n",
    "    y_test = best[\"y_test\"]\n",
    "    pred = best[\"pred\"]\n",
    "    prob = best[\"prob\"]\n",
    "    metrics = best[\"metrics\"]\n",
    "    thresholds = best[\"label_thresholds\"]\n",
    "    \n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "    bal_acc = balanced_accuracy_score(y_test, pred)\n",
    "    \n",
    "    print(\"Balanced Accuracy:\", bal_acc)\n",
    "    print(\"\\nSelected Label Thresholds:\", thresholds)\n",
    "    print(\"Test Metrics:\", metrics)\n",
    "\n",
    "    # Save metrics\n",
    "    with open(os.path.join(OUTDIR, \"rf_metrics.json\"), \"w\") as f:\n",
    "        json.dump({\"thresholds\": thresholds, \"metrics\": metrics}, f, indent=2)\n",
    "\n",
    "    # Save scored test set\n",
    "    scored_test = test_df.copy()\n",
    "    scored_test[\"pred_label\"] = pred\n",
    "    scored_test[\"prob_irregular\"] = prob\n",
    "    scored_test.to_csv(os.path.join(OUTDIR, \"rf_scored_test_days.csv\"), index=False)\n",
    "\n",
    "    # -------- Visualizations --------\n",
    "    plot_label_distribution(make_labels_percentile(daily, top_q=thresholds[\"top_q\"]),\n",
    "                        os.path.join(OUTDIR, \"label_distribution.png\"))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal(0)\", \"Irregular(1)\"])\n",
    "    disp.plot(values_format=\"d\")\n",
    "    plt.title(\"Confusion Matrix (Test)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTDIR, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # ROC Curve (only meaningful if both classes present)\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        fpr, tpr, _ = roc_curve(y_test, prob)\n",
    "        auc = roc_auc_score(y_test, prob)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "        plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve (Test)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTDIR, \"roc_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Precision-Recall\n",
    "        precs, recs, _ = precision_recall_curve(y_test, prob)\n",
    "        ap = average_precision_score(y_test, prob)\n",
    "        plt.figure()\n",
    "        plt.plot(recs, precs, label=f\"AP = {ap:.3f}\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(\"Precisionâ€“Recall Curve (Test)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTDIR, \"pr_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Feature importance\n",
    "    plot_feature_importance(rf, FEATURES, os.path.join(OUTDIR, \"feature_importance.png\"), top_n=15)\n",
    "\n",
    "    # Probability timeline\n",
    "    plot_prob_timeline(scored_test, prob, os.path.join(OUTDIR, \"probability_timeline.png\"))\n",
    "\n",
    "    # Save model + scaler\n",
    "    import joblib\n",
    "    joblib.dump(rf, os.path.join(OUTDIR, \"rf_model.pkl\"))\n",
    "    joblib.dump(scaler, os.path.join(OUTDIR, \"rf_scaler.pkl\"))\n",
    "\n",
    "    # Print a clean report\n",
    "    print(\"\\nClassification Report (Test):\")\n",
    "    print(classification_report(y_test, pred, target_names=[\"Normal(0)\", \"Irregular(1)\"], zero_division=0))\n",
    "\n",
    "    print(\"\\nâœ… DONE. Outputs saved in:\", OUTDIR)\n",
    "    print(\"Key charts:\",\n",
    "          \"label_distribution.png, confusion_matrix.png, roc_curve.png (if valid), pr_curve.png (if valid),\",\n",
    "          \"feature_importance.png, probability_timeline.png\", sep=\"\\n- \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d551d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FuelWatch ML (venv)",
   "language": "python",
   "name": "fuelwatch-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
